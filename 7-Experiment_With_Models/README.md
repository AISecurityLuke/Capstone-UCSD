# Production-Grade ML Experiment Framework for Prompt Classification

A comprehensive, enterprise-ready machine learning experiment framework for prompt classification with support for traditional ML, deep learning, and transformer models. Built with production-grade features including model evaluation, A/B testing, deployment pipelines, and comprehensive monitoring.

## ‚úÖ Alignment with Capstone Step 7 and Step 8

This section explicitly maps the repository to the Capstone rubric for:

- Step 7: Experiment with Various Models
- Step 8: Scale Your Prototype with Large-Scale Data

Links to rubric references: see the program‚Äôs Step 7 and Step 8 descriptions.

### Step 7 ‚Äî Experiment with Various Models

- Model breadth: Traditional ML (LogReg, RF, SVM, NB, XGBoost), Deep Learning (CNN, LSTM, BiLSTM, Hybrid), Transformers (BERT, DistilBERT, RoBERTa, ALBERT, DistilRoBERTa, XLM-RoBERTa). Enable/disable via `config/config.json`.
- Comparison and selection: Unified metrics written to `results/results.csv` and consolidated visualizations in `results/images/`. Custom security-focused metrics implemented in `utils/metrics.py`.
- Documentation: This README provides setup, configuration, architecture, and usage details. Analysis scripts: `results/visual.py`, `results/dl_visualizations.py`, `results/training_analysis.py`.
- Reproducibility: Seed control via `run_pipeline.py` ‚Üí `utils/reproducibility.set_global_seed` and config.

Outcome: Step 7 is fully satisfied.

### Step 8 ‚Äî Scale Your Prototype with Large-Scale Data

Learning objectives addressed:

- Handles complete dataset: Trained on the full combined dataset (‚âà30,000 samples). Evidence and how to verify are provided below.
- Demonstrated scaling capability: Configurable tokenization workers, dataloader workers, batch sizes, sequence lengths, CV folds, early stopping, and optional distributed settings. Results and timing captured in logs/stdout and consolidated into `results/results.csv`.
- Trade-offs analysis: See the ‚ÄúScaling Trade-offs Analysis‚Äù section below, tied directly to `config/config.json`.
- Choice of tools/libraries and techniques: scikit-learn for classical models, TensorFlow/Keras for DL, HuggingFace Transformers for LLM fine-tuning; XGBoost and Optuna supported. Choices explained below.
- Documentation: End-to-end runbook below; notebooks referenced for step-by-step walkthrough.

Outcome: Step 8 requirements are satisfied with explicit evidence and analysis. See checklist for evaluators at the end of this section.

### Metric choices and selection criteria

*We use a custom scoring system designed specifically for security classification.*
The main components are:

1. Base Score (weighted combination):
   - 50% weight on recall for malicious class (catching threats)
   - 30% weight on precision for suspicious class (reducing false alerts)
   - 20% weight on F1 score for benign class (overall benign accuracy)

2. Security Penalty:
   - We track critical misclassifications between benign and malicious classes
   - An exponential penalty reduces the score when these errors occur
   - Even a small rate of critical errors will significantly impact the final score

### Scaling Evidence (30k samples) and How to Verify

- Dataset: `Capstone-UCSD/5-Data_Wrangling/combined.json` (configured in `config/config.json` ‚Üí `data_path`).
- Configuration used (example):
  - `traditional_ml.cv_folds = 4`
  - `deep_learning.max_epochs = 25`, `batch_size = 64`, `early_stopping_patience = 5`
  - `transformers.max_epochs = 15`, `batch_size = 16`, `tokenizers_parallelism = true`, `tokenization_num_proc = 4`, `dataloader_num_workers = 4`
  - `preprocessing.tokenization.max_length = 1024` (Keras/RNN/CNN) and 512 for HF models.
- Command:
  - `python run_pipeline.py`
- Where to look:
  - Results summary: `results/results.csv` (one row per trained model, includes performance columns and status)
  - Detailed validation artifacts: multiple `results/validation_*.json` files
  - Visuals: `results/images/`

Note: `config/config.json` sets `max_train_samples: null` (no downsampling) to ensure full-dataset training.

### Scaling Trade-offs Analysis (tied to config)

- Batch size vs memory/time (`deep_learning.batch_size`, `transformers.batch_size`)
  - Larger batches improve throughput but increase peak memory. Empirically, `64` (DL) and `16` (HF) maximize throughput on 16 GB systems without OOM.
- Sequence length vs accuracy/latency (`preprocessing.tokenization.max_length`, HF defaults to 512)
  - Longer sequences improve recall for long prompts; training/inference time grows roughly linearly with length for tokenization and quadratically with transformer attention.
- Cross-validation folds vs wall-time (`traditional_ml.cv_folds`, `transformers.cv_folds`)
  - More folds reduce variance but increase compute linearly. Chosen `4` as balance; increase to `5‚Äì10` for final stability if wall-time allows.
- Model complexity vs scalability
  - Classical models scale near-linearly with feature count; fast to train and serve.
  - CNN/LSTM scale with sequence length and embedding size; early stopping controls overfitting and training time.
  - Transformers deliver highest recall but demand most memory/time; use smaller variants (Distil*, ALBERT) and FP16 (`transformers.fp16 = true`) to reduce cost.
- Parallelism knobs
  - Tokenization (`transformers.tokenization_num_proc`) and DataLoader (`transformers.dataloader_num_workers`) improve CPU utilization for large corpora.
  - Disable parallel tokenizers when contention occurs (`TOKENIZERS_PARALLELISM=false`).

Rationale for tool choices:

- scikit-learn: robust baselines and fair comparisons for tabularized text (TF-IDF).
- TensorFlow/Keras: flexible DL architectures (CNN/LSTM/Hybrid) with mature callbacks (EarlyStopping, ReduceLROnPlateau).
- HuggingFace Transformers: state-of-the-art text models with Trainer API, mixed precision, and well-tested tokenizers.
- XGBoost: strong non-linear baseline; Optuna-compatible for efficient HPO.

### Runbook: Full-Dataset and Scaling Variants

1) Full-dataset run (recommended defaults)

```
python run_pipeline.py
```

2) Faster iteration (reduced compute)

Set in `config/config.json`:

```
"traditional_ml": { "cv_folds": 2 },
"deep_learning": { "max_epochs": 8, "batch_size": 32 },
"transformers": { "max_epochs": 4, "batch_size": 8, "logging_steps": 100 }
```

3) Higher-throughput tokenization and loading

```
"transformers": {
  "tokenizers_parallelism": true,
  "tokenization_num_proc": 4,
  "dataloader_num_workers": 4,
  "fp16": true
}
```

4) Longer-context experiments (accuracy focus)

```
"preprocessing": { "tokenization": { "max_length": 1024 } }
```

5) Optional: Classical-only or DL-only ablations

Edit `traditional_ml.models`, `deep_learning.models`, `transformers.models` arrays to target specific families and compare scaling/performance.

### Notebook Walkthroughs

- Data wrangling and exploration: `Capstone-UCSD/5-Data_Wrangling/data_processing_demo.ipynb`
  - Shows schema, cleaning, and preparation steps that feed this pipeline.

If desired, a slim ‚ÄúExperiment Walkthrough‚Äù notebook can call `experiment/experiment_runner.py` with a chosen config to provide a cell-by-cell narrative. The CLI path above is the canonical entrypoint used for results.

### Evaluator Checklist (Mapping to Rubric)

- Code is on GitHub: repository contains complete, runnable source and configuration.
- Understanding of scaling: ‚ÄúScaling Trade-offs Analysis‚Äù and configuration knobs documented above.
- Scaled prototype handles complete dataset: full run executed on ‚âà30k samples; artifacts in `results/`.
- Tools/libraries justified: see ‚ÄúRationale for tool choices‚Äù.
- Technique choices justified: classical vs DL vs Transformers trade-offs discussed; small/efficient transformer variants enabled.
- Documentation: README + analysis scripts, optional notebook reference.


## üîÅ Reproducibility & Verification Guide

Follow these exact steps to reproduce the results and verify rubric items for Step 7 and Step 8.

### 1) Environment setup

```
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

Notes:
- On Apple Silicon, TensorFlow wheels may require `tensorflow-macos` and `tensorflow-metal`; if necessary, install per Apple TF instructions.
- If MLflow or W&B are not desired, they are disabled by default in `run_pipeline.py` using envs.

### 2) Data availability

Ensure the dataset file exists at the configured path:

```
Capstone-UCSD/5-Data_Wrangling/combined.json
```

`config/config.json` uses:

```
"data_path": "../5-Data_Wrangling/combined.json"
```

### 3) Run the full pipeline

```
python run_pipeline.py
```

This will:
- Load and validate data
- Train enabled models (traditional ML, DL, transformers per config)
- Save metrics and artifacts to `results/`

### 4) Verify artifacts

Check the following after a successful run:
- `results/results.csv` contains one row per model with standard metrics and status
- `results/validation_*.json` files exist with per-model validation details
- `results/images/` contains comparison and training visuals
- `logs/` contains the detailed `experiment.log`

### 5) Verify custom metrics

Confirm presence of custom metrics (when computed):
- `custom_score`, `recall_c2`, `precision_c1`, `f1_c0`, and critical error rates.
- See implementation at `utils/metrics.py` and check logs for printed values.

### 6) Scaling evidence

The run uses the full dataset (~30k samples) as configured (`max_train_samples: null`).
- Inspect runtime and memory logs (`logs/`, stdout) and confirm successful completion.
- Tokenization/loader workers are configurable in `config/config.json` under `transformers`.

### 7) Quick variations (optional)

- Faster iteration: reduce epochs/batch size and CV folds (see Runbook section).
- Longer context: increase `preprocessing.tokenization.max_length`.

### 8) Rubric mapping checks

- Step 7: Multiple model families trained; comparative results in `results/results.csv`; visuals under `results/images/`; custom metrics documented.
- Step 8: Full dataset processed; scaling knobs documented; trade-offs analysis included; reproduction steps above.



**üîí Security-Focused Design**: This framework is specifically optimized for malicious prompt detection with custom metrics that heavily penalize misclassifications between benign (class 0) and malicious (class 2) content, ensuring high recall for threats while minimizing false alarms.

## üöÄ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt

# Run the complete production pipeline
python run_pipeline.py          # convenience wrapper
# (equivalent to: python experiment/experiment_runner.py)
```

This single command will:
1. **Load and validate data** with comprehensive quality checks
2. **Train all configured models** across three categories (Traditional ML, Deep Learning, Transformers)
3. **Perform rigorous evaluation** with cross-validation, statistical significance testing, **and an external blind-set validation** (column `f1_external` in `results/results.csv`)
4. **Generate comprehensive visualizations** and insights
5. **Select the best model** using multi-criteria optimization with custom security-focused scoring
6. **Prepare deployment artifacts** with model cards and documentation

## üèóÔ∏è Architecture Overview

```
7-Experiment_With_Models/
‚îú‚îÄ‚îÄ config/                     # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ config.json            # Production ML configuration
‚îÇ   ‚îî‚îÄ‚îÄ config.py              # Configuration loader
‚îú‚îÄ‚îÄ data/                      # Data processing pipeline
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py         # Data loading and preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ validation/            # Data quality validation
‚îú‚îÄ‚îÄ training/                  # Training modules
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py             # Traditional ML (sklearn)
‚îÇ   ‚îú‚îÄ‚îÄ adaptive_trainer.py    # Deep Learning (Keras/TensorFlow)
‚îÇ   ‚îî‚îÄ‚îÄ transformer_trainer.py # Transformers (HuggingFace)
‚îú‚îÄ‚îÄ models/                    # Model definitions and storage
‚îÇ   ‚îú‚îÄ‚îÄ keras_models.py        # Keras model factory
‚îÇ   ‚îî‚îÄ‚îÄ [model_directories]/   # Trained model artifacts
‚îú‚îÄ‚îÄ experiment/                # Experiment orchestration
‚îÇ   ‚îî‚îÄ‚îÄ experiment_runner.py   # Main experiment runner
‚îú‚îÄ‚îÄ validation_jsons/          # Small blind-set datasets for external validation
‚îÇ   ‚îî‚îÄ‚îÄ outside_test_data.json # Default 96-prompt test set
‚îú‚îÄ‚îÄ run_pipeline.py            # Thin wrapper that sets cwd & launches experiment
‚îú‚îÄ‚îÄ results/                   # Output and analysis
‚îÇ   ‚îú‚îÄ‚îÄ visual.py              # Standard visualizations
‚îÇ   ‚îú‚îÄ‚îÄ dl_visualizations.py   # Deep learning specific viz
‚îÇ   ‚îî‚îÄ‚îÄ training_analysis.py   # Training analysis
‚îú‚îÄ‚îÄ utils/                     # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py             # Custom security-focused metrics
‚îÇ   ‚îî‚îÄ‚îÄ focal_loss.py          # Focal loss implementations
‚îú‚îÄ‚îÄ logs/                      # Experiment logs
‚îî‚îÄ‚îÄ requirements.txt           # Dependencies
```

## üìä Model Categories

### 1. **Traditional Machine Learning**
- **Logistic Regression**: Linear classification with regularization
- **Random Forest**: Ensemble tree-based classification
- **Support Vector Machine**: Kernel-based classification
- **Naive Bayes**: Probabilistic classification
- **XGBoost**: Gradient boosting for classification

### 2. **Deep Learning (Keras/TensorFlow)**
- **CNN**: Convolutional Neural Networks for text classification
- **LSTM**: Long Short-Term Memory networks
- **BiLSTM**: Bidirectional LSTM networks
- **Transformer**: Attention-based transformer models
- **Hybrid**: Combined architectures (CNN + LSTM)

### 3. **Transformers (HuggingFace)**
- **BERT**: Bidirectional Encoder Representations from Transformers
- **DistilBERT**: Distilled version of BERT (40% smaller, 60% faster)
- **RoBERTa**: Robustly Optimized BERT Pretraining Approach
- **ALBERT**: A Lite BERT (very lightweight, 45MB)
- **DistilRoBERTa**: Distilled version of RoBERTa
- **XLM-RoBERTa**: Cross-lingual transformer models

## üîí Security-Focused Custom Metrics

### **Custom Scoring System**
The framework uses a specialized scoring system designed for security applications:

```python
# Base score components (weighted by importance)
base_score = 0.5 * recall_c2 + 0.3 * precision_c1 + 0.2 * f1_c0

# Critical misclassification penalty (exponential)
critical_error_rate = P(0‚Üí2) + P(2‚Üí0)  # benign‚Üímalicious + malicious‚Üíbenign
penalty_factor = exp(-5 * critical_error_rate)

# Final custom score
custom_score = base_score * penalty_factor
```

### **Security Objectives**
1. **Maximize malicious recall** (class 2): Catch as many threats as possible
2. **Minimize false alarms** (0‚Üí2): Almost never block legitimate prompts
3. **Route uncertainty to suspicious** (class 1): Let SIEM handle edge cases
4. **Maintain suspicious precision**: Avoid analyst fatigue

### **Critical Error Penalties**
- **Exponential penalty scaling**: Even small rates of 0‚Üî2 misclassifications severely impact scores
- **Separate tracking**: Monitor false alarms (0‚Üí2) vs missed threats (2‚Üí0) independently
- **Business alignment**: Penalties reflect the high cost of security failures

## üìä Results and Metrics

### **Results CSV Structure**
```
results/results.csv columns:
‚îú‚îÄ‚îÄ model                    # Model name
‚îú‚îÄ‚îÄ precision_macro          # Overall precision
‚îú‚îÄ‚îÄ recall_macro            # Overall recall  
‚îú‚îÄ‚îÄ f1_macro               # Overall F1 score
‚îú‚îÄ‚îÄ roc_auc                # ROC AUC score
‚îú‚îÄ‚îÄ best_params            # Best hyperparameters
‚îú‚îÄ‚îÄ status                 # Training status
‚îú‚îÄ‚îÄ f1_external           # External blind-set F1
‚îú‚îÄ‚îÄ precision_c1          # Precision for suspicious class
‚îú‚îÄ‚îÄ recall_c2             # Recall for malicious class
‚îú‚îÄ‚îÄ f1_c0                 # F1 for benign class
‚îú‚îÄ‚îÄ custom_score          # Security-focused composite score
‚îú‚îÄ‚îÄ false_alarm_rate      # Rate of benign‚Üímalicious errors
‚îú‚îÄ‚îÄ missed_threat_rate    # Rate of malicious‚Üíbenign errors
‚îî‚îÄ‚îÄ per_class_f1          # Per-class F1 scores
```

### **Key Metrics to Monitor**
- **`custom_score`**: Primary metric for model selection (higher is better)
- **`recall_c2`**: Malicious threat detection rate (target: >0.9)
- **`false_alarm_rate`**: Benign‚Üímalicious errors (target: <0.01)
- **`missed_threat_rate`**: Malicious‚Üíbenign errors (target: <0.05)
- **`precision_c1`**: Suspicious classification precision (target: >0.7)
- **`f1_external`**: Performance on unseen blind-set (target: >0.6)

### **Model Selection Criteria**
1. **Security Performance**: `custom_score` > 0.6, `recall_c2` > 0.85
2. **False Alarm Control**: `false_alarm_rate` < 0.02
3. **Threat Detection**: `missed_threat_rate` < 0.1
4. **Efficiency**: Latency < 100ms, memory < 2GB
5. **Stability**: Cross-validation std < 0.05

## üîß Configuration

### **Security-Focused Configuration**

#### **Class Weights and Loss Functions**
```json
{
  "use_class_weights": true,
  "class_weights": [1, 1, 1.5],  // [benign, suspicious, malicious]
  "deep_learning": {
    "loss_function": "focal",
    "focal_gamma": 3.0,
    "focal_alpha": [1, 1, 1.5]
  },
  "transformers": {
    "loss_function": "focal", 
    "focal_gamma": 3.0,
    "focal_alpha": [1, 1, 1.5]
  }
}
```

#### **Custom Metrics Configuration**
```json
{
  "custom_metric": true,  // Enable custom scoring for model selection
  "external_validation": {
    "enable": true,
    "data_path": "validation_jsons/outside_test_data.json"
  }
}
```

### **Production-Grade Configuration Features**

The framework includes comprehensive configuration for production ML:

#### **Model Evaluation**
```json
{
  "model_evaluation": {
    "enable_cross_validation": true,
    "cv_folds": 4,
    "stratified_sampling": true,
    "metrics": ["accuracy", "precision_macro", "recall_macro", "f1_macro", "roc_auc"],
    "confidence_intervals": true,
    "statistical_significance_test": true,
    "performance_thresholds": {
      "min_accuracy": 0.7,
      "min_f1": 0.6,
      "max_latency_ms": 100
    }
  }
}
```

## üöÄ Usage Examples

### **Basic Usage**
```bash
# Run complete pipeline with all models
python experiment/experiment_runner.py
```

### **Custom Configuration**
```python
from experiment.experiment_runner import ProductionExperimentRunner
from config.config import load_config

# Load custom configuration
config = load_config("custom_config.json")

# Initialize and run experiment
runner = ProductionExperimentRunner(config)
results = runner.run_experiment()

# Access results
best_model = results.get_best_model()
performance_metrics = results.get_performance_summary()
```

### **Model-Specific Training**
```json
{
  "traditional_ml": {
    "models": ["logistic_regression", "random_forest"],
    "cv_folds": 10
  },
  "deep_learning": {
    "models": ["cnn", "lstm"],
    "max_epochs": 20
  },
  "transformers": {
    "models": ["distilbert"],
    "max_epochs": 5
  }
}
```

### **Ensemble Training**
```json
{
  "model_selection": {
    "ensemble_methods": {
      "enable_voting": true,
      "enable_stacking": true,
      "voting_strategy": "soft"
    }
  }
}
```

## üìä Output Interpretation

### **Results Structure**
```
results/
‚îú‚îÄ‚îÄ results.csv              # Complete experiment results with custom metrics
‚îú‚îÄ‚îÄ experiment_results.json  # Detailed results with metadata
‚îú‚îÄ‚îÄ images/                  # Comprehensive visualizations
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.png
‚îÇ   ‚îú‚îÄ‚îÄ training_curves.png
‚îÇ   ‚îú‚îÄ‚îÄ feature_importance.png
‚îÇ   ‚îî‚îÄ‚îÄ fairness_analysis.png
‚îî‚îÄ‚îÄ models/                  # Trained model artifacts
    ‚îú‚îÄ‚îÄ [model_name]/
    ‚îÇ   ‚îú‚îÄ‚îÄ model.pkl
    ‚îÇ   ‚îú‚îÄ‚îÄ config.json
    ‚îÇ   ‚îî‚îÄ‚îÄ performance.json
```

## üîç Troubleshooting

### **Common Issues & Solutions**

#### **Memory Errors**
```bash
# Reduce batch size in config
"batch_size": 16  # Instead of 64
"max_length": 256  # Instead of 512
```

#### **Slow Training**
```bash
# Enable GPU acceleration
"enable_gpu": true
"mixed_precision": true

# Reduce model complexity
"layers": 1  # Instead of 3
"units": 64  # Instead of 256
```

#### **Poor Security Performance**
- Check class balance in training data
- Verify external validation set quality
- Consider adjusting class weights or focal loss parameters
- Review false alarm vs missed threat rates separately

#### **Transformer-Specific Issues**
- Ensure MLflow is uninstalled (causes conflicts)
- Use smaller batch sizes for M1/M2 Macs
- Monitor memory usage during training

## üìã Requirements

```bash
pip install -r requirements.txt
```

### **Core Dependencies**
- `tensorflow>=2.8.0` - Deep learning framework
- `scikit-learn>=1.0.0` - Traditional ML
- `transformers>=4.20.0` - HuggingFace transformers
- `pandas>=1.3.0` - Data manipulation
- `numpy>=1.21.0` - Numerical computing
- `matplotlib>=3.5.0` - Visualization
- `seaborn>=0.11.0` - Statistical visualization

### **Production Dependencies**
- `mlflow>=1.20.0` - Experiment tracking
- `shap>=0.40.0` - Model interpretability
- `optuna>=2.10.0` - Hyperparameter optimization
- `prometheus-client>=0.12.0` - Monitoring
- `docker>=6.0.0` - Containerization

## üöÄ Deployment

### **Docker Deployment**
```bash
# Build container
docker build -t prompt-classifier .

# Run container
docker run -p 8080:8080 prompt-classifier
```

### **Kubernetes Deployment**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prompt-classifier
spec:
  replicas: 3
  selector:
    matchLabels:
      app: prompt-classifier
  template:
    metadata:
      labels:
        app: prompt-classifier
    spec:
      containers:
      - name: classifier
        image: prompt-classifier:latest
        ports:
        - containerPort: 8080
        resources:
          limits:
            cpu: "1000m"
            memory: "2Gi"
```

## üìù Contributing

### **Development Guidelines**
1. **Modular Architecture**: Keep components loosely coupled
2. **Comprehensive Testing**: Unit tests for all modules
3. **Documentation**: Update README and docstrings
4. **Configuration**: Add new options to config.json
5. **Logging**: Use structured logging throughout
6. **Security Focus**: Maintain custom metrics and penalty system

### **Code Quality**
- Follow PEP 8 style guidelines
- Add type hints to all functions
- Include comprehensive error handling
- Write unit tests for new features
- Update configuration documentation
- Ensure custom metrics are computed for all model types

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ü§ù Acknowledgments

- **HuggingFace** for transformer models and training utilities
- **TensorFlow/Keras** for deep learning framework
- **Scikit-learn** for traditional machine learning
- **MLflow** for experiment tracking and model management

---

# ML Experiment Framework for Prompt Classification

A comprehensive, production-ready machine learning experiment framework for prompt classification with support for traditional ML and deep learning models.

## üöÄ Quick Start

```bash
# Run the complete pipeline
python run_pipeline.py
```

This single command will:
1. Load and validate data
2. Train all configured models (traditional ML + deep learning)
3. Generate comprehensive visualizations
4. Create training analysis reports
5. Generate deep learning specific insights

## üìä Generated Outputs

After running the experiment, you'll find:

### Visualizations (`results/images/`)
- `model_comparison.png` - Overall model performance comparison
- `radar_chart.png` - Top 5 models radar chart
- `parallel_coordinates.png` - Multi-dimensional model comparison
- `training_analysis.png` - Training progress analysis
- `convergence_analysis.png` - Model convergence patterns

### Deep Learning Visualizations (`results/images/`)
- `dl_training_curves.png` - Detailed DL training analysis (6 subplots)
- `dl_architecture_comparison.png` - Architecture-specific comparisons
- `dl_insights.txt` - DL-specific recommendations and insights

### Training Analysis (`results/images/`)
- `training_analysis.png` - Training curves and convergence analysis
- `training_insights.txt` - Automated training insights and recommendations

### Data Files
- `results.csv` - Complete experiment results
- `experiment.log` - Detailed experiment log

## üèóÔ∏è Architecture

```
7-Experiment_With_Models/
‚îú‚îÄ‚îÄ config/                 # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ config.json        # Main configuration file
‚îÇ   ‚îî‚îÄ‚îÄ config.py          # Configuration loader
‚îú‚îÄ‚îÄ data/                  # Data processing
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py     # Data loading and preprocessing
‚îÇ   ‚îî‚îÄ‚îÄ validation/        # Data validation modules
‚îú‚îÄ‚îÄ training/              # Training modules
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py         # Traditional ML trainer
‚îÇ   ‚îî‚îÄ‚îÄ adaptive_trainer.py # Deep learning trainer
‚îú‚îÄ‚îÄ models/                # Model definitions
‚îÇ   ‚îî‚îÄ‚îÄ keras_models.py    # Keras model factory
‚îú‚îÄ‚îÄ experiment/            # Experiment orchestration
‚îÇ   ‚îî‚îÄ‚îÄ experiment_runner.py # Main experiment runner
‚îú‚îÄ‚îÄ results/               # Output and analysis
‚îÇ   ‚îú‚îÄ‚îÄ visual.py          # Standard visualizations
‚îÇ   ‚îú‚îÄ‚îÄ dl_visualizations.py # Deep learning specific viz
‚îÇ   ‚îî‚îÄ‚îÄ training_analysis.py # Training analysis
‚îú‚îÄ‚îÄ utils/                 # Utility modules
‚îú‚îÄ‚îÄ logs/                  # Experiment logs
‚îú‚îÄ‚îÄ mlruns/                # MLflow tracking
‚îî‚îÄ‚îÄ run_pipeline.py      # Main entry point
```

## üîß Configuration

### Main Configuration (`config/config.json`)

```json
{
  "data": {
    "data_path": "path/to/your/data",
    "test_size": 0.2,
    "random_state": 42,
    "max_features": 10000
  },
  "models": [
    {
      "name": "logistic_regression",
      "type": "sklearn",
      "params": {
        "C": [0.1, 1, 10],
        "penalty": ["l1", "l2"]
      }
    },
    {
      "name": "cnn_default",
      "type": "cnn",
      "params": {
        "filters": [32, 64],
        "kernel_size": [3, 5],
        "dropout": [0.3, 0.5]
      }
    }
  ],
  "training": {
    "epochs": 10,
    "batch_size": 32,
    "validation_split": 0.2,
    "early_stopping": true,
    "patience": 3
  },
  "experiment_tracking": {
    "enable_mlflow": true,
    "enable_wandb": false,
    "experiment_name": "prompt_classification"
  }
}
```

### Deep Learning Models

The framework supports various deep learning architectures:

- **CNN**: Convolutional Neural Networks for text classification
- **LSTM**: Long Short-Term Memory networks
- **BiLSTM**: Bidirectional LSTM networks
- **Transformer**: Attention-based transformer models
- **Hybrid**: Combined architectures (CNN + LSTM)
- **BERT**: Bidirectional Encoder Representations from Transformers
- **DistilBERT**: Distilled version of BERT (40% smaller, 60% faster)
- **RoBERTa**: Robustly Optimized BERT Pretraining Approach
- **ALBERT**: A Lite BERT (very lightweight, 45MB)
- **DistilRoBERTa**: Distilled version of RoBERTa

Each architecture can be configured with:
- Layer configurations (filters, units, heads)
- Dropout rates
- Learning rate schedules
- Regularization parameters

## üìà Deep Learning Visualizations

The framework includes specialized deep learning visualizations:

### Training Curves Analysis
- **Loss Curves**: Training vs validation loss with gradient analysis
- **Accuracy Curves**: Training vs validation accuracy progression
- **Learning Rate**: Learning rate scheduling visualization
- **Gradient Analysis**: Loss change rate and stability
- **Overfitting Detection**: Train-validation gap analysis
- **Stability Metrics**: Rolling loss standard deviation

### Architecture Comparison
- **Performance by Architecture**: Best accuracy comparison
- **Convergence Speed**: Epochs to 90% of best performance
- **Stability Analysis**: Loss variance vs performance
- **Overfitting Analysis**: Train-val gap vs performance

### Automated Insights
The system generates architecture-specific recommendations:
- CNN: Filter/layer optimization suggestions
- LSTM: Bidirectional and unit count recommendations
- Transformer: Attention head and layer suggestions
- BERT: Fine-tuning and learning rate recommendations
- DistilBERT: Performance vs efficiency trade-offs
- RoBERTa: Advanced fine-tuning strategies
- ALBERT: Lightweight model optimization
- DistilRoBERTa: Distillation-specific insights
- General: Learning rate, regularization, and convergence tips

## üîç Training Analysis Features

### Automated Insights
- **Convergence Analysis**: Identifies optimal stopping points
- **Overfitting Detection**: Flags models with high train-val gaps
- **Learning Rate Analysis**: Suggests scheduling improvements
- **Stability Assessment**: Identifies models with high variance

### Performance Metrics
- **Training Curves**: Loss and accuracy over epochs
- **Convergence Patterns**: Time to convergence analysis
- **Resource Utilization**: Memory and time tracking
- **Model Comparison**: Cross-model performance analysis

## üöÄ Advanced Features

### Experiment Tracking
- **MLflow Integration**: Comprehensive experiment tracking
- **Weights & Biases**: Optional W&B integration
- **Model Registry**: Versioned model storage
- **Artifact Logging**: Save models, plots, and data

### Data Validation
- **Schema Validation**: Ensures data format consistency
- **Quality Validation**: Checks for data quality issues
- **Automated Warnings**: Flags potential problems

### Resource Monitoring
- **Memory Tracking**: Monitor memory usage during training
- **Time Profiling**: Track training and inference times
- **GPU Utilization**: Monitor GPU usage for deep learning

### Distributed Training
- **Ray Integration**: Optional distributed training support
- **Multi-GPU**: Automatic multi-GPU training
- **Hyperparameter Tuning**: Distributed hyperparameter search

## üìã Requirements

```bash
pip install -r requirements.txt
```

Key dependencies:
- `tensorflow>=2.8.0`
- `scikit-learn>=1.0.0`
- `pandas>=1.3.0`
- `matplotlib>=3.5.0`
- `seaborn>=0.11.0`
- `mlflow>=1.20.0`
- `numpy>=1.21.0`

## üéØ Usage Examples

### Basic Experiment
```bash
python run_pipeline.py
```

### Custom Configuration
```python
from experiment.experiment_runner import ProductionExperimentRunner
from config.config import load_config

config = load_config("custom_config.json")
runner = ProductionExperimentRunner(config)
runner.run_experiment()
```

### Deep Learning Focus
```json
{
  "models": [
    {"name": "cnn_best", "type": "cnn"},
    {"name": "lstm_best", "type": "lstm"},
    {"name": "transformer_best", "type": "transformer"}
  ],
  "training": {
    "epochs": 20,
    "early_stopping": true
  }
}
```

## üìä Output Interpretation

### Standard Visualizations
- **Model Comparison**: Overall performance ranking
- **Radar Chart**: Multi-metric comparison of top models
- **Parallel Coordinates**: Multi-dimensional model analysis

### Deep Learning Insights
- **Training Stability**: Look for smooth, decreasing loss curves
- **Overfitting**: Watch for increasing train-val gaps
- **Convergence**: Identify optimal stopping points
- **Architecture Performance**: Compare different DL architectures

### Recommendations
- **High Variance**: Reduce learning rate or add regularization
- **Overfitting**: Add dropout or implement early stopping
- **Slow Convergence**: Increase learning rate or model capacity
- **Poor Performance**: Consider architecture changes or more data

## üîß Troubleshooting

### Common Issues
1. **Memory Errors**: Reduce batch size or model complexity
2. **Slow Training**: Enable GPU acceleration or reduce data size
3. **Poor Performance**: Check data quality and model configuration
4. **Visualization Errors**: Ensure matplotlib backend is properly configured

### Debug Mode
```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üìù Contributing

1. Follow the modular architecture
2. Add comprehensive logging
3. Include data validation
4. Update configuration documentation
5. Add tests for new features

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## Transformer Training (HuggingFace)

`training/transformer_trainer.py` handles all HuggingFace models (BERT, DistilBERT, RoBERTa, etc.).  Key features:

* Separate from Keras/DL flow ‚Äì keeps dependencies light.
* Uses `TrainingArguments` with:
  * Linear LR scheduler with 10% warm-up.
  * Weight decay (0.01) and AdamW optimizer.
  * Gradient clipping (`max_grad_norm=1.0`).
* Automatically saves best checkpoints and logs metrics to `results/results.csv`.

The `experiment_runner` now calls three distinct pipelines:

1. **Traditional ML** (`training/trainer.py`)
2. **Deep-learning / Keras** (`training/adaptive_trainer.py`)
3. **Transformers** (`training/transformer_trainer.py`)

Results from all pipelines are aggregated in the final summary.

## üîë Default Configuration & Rationale  
Below is **why** the numbers in `config/config.json` are set the way they are.  These values represent a balance between statistical robustness and wall-time on a single Apple-Silicon laptop with 16 K samples.

| Area | Default | Why it's the sweet-spot |
|------|---------|-------------------------|
| **Traditional ML** | `cv_folds = 10`, timeout = 1200 s | 10-fold CV reduces the standard-error ~30 % vs 5 folds while staying well under 1 h total runtime.<br>Each model runs **GridSearchCV** with hard-coded grids in `training/trainer.py` (LogReg, RF, SVM-linear, NB, XGB). |
| **Deep-Learning** | `max_epochs = 15`, `batch_size = 64`, `early_stopping_patience = 3` | 15 is an upper bound; Early-Stopping usually halts at 7‚Äì10 epochs. Batch 64 fits comfortably in M-series RAM and gives GPU/ANE utilisation. |
| **Transformers** | `max_epochs = 8`, `batch_size = 16`, `early_stopping_patience = 3` | Fine-tuning converges in 3‚Äì6 epochs; 8 gives head-room. Batch 16 prevents OOM on 16 GB Macs. |
| **Tokenisation** | `max_length = 512` for HF models, 1024 for Keras RNN/CNN | BERT-style models max at 512; CNN/LSTM can benefit from longer sequences. |
| **Hyper-parameter search** | GridSearchCV for classical models **on**, Optuna **off by default** | The built-in grids already cover the typical sweet-spots. Turn on Optuna only when you need deeper sweeps. |
| **Safety & Logging** | `utils/sanitizer.sanitize_text` applied **before** any tokenisation | Guarantees the same text preprocessing in both training and inference and neutralises XSS/RTL/Excel-injection payloads. |

### End-to-End Flow
```text
raw JSON ‚ûú data_loader (sanitize + label-encode + split)
          ‚ûú trainer.py              (TF-IDF + GridSearchCV, 10-fold)
          ‚ûú adaptive_trainer.py     (Keras models, Early-Stopping)
          ‚ûú transformer_trainer.py  (HF fine-tuning, Early-Stopping)
          ‚ûú results/results.csv     (one row per model)
          ‚ûú results/*.py            (visuals + insights)
```
‚Ä¢ All three trainers append their metrics to **the same** CSV so downstream plots compare apples-to-apples.  
‚Ä¢ Logs land in `logs/` and are parsed by `training_analysis.py` / `dl_visualizations.py` for deeper diagnostics.

### Modifying the defaults
* **Faster iteration**‚ÄÉLower `traditional_ml.cv_folds` to 5 and set `transformers.max_epochs` to 4.  
* **Exhaustive sweep**‚ÄÉTurn `hyperparameter_optimization.enable_optuna` to `true` and define search spaces for every model.  
* **Memory-constrained**‚ÄÉDrop `batch_size` for DL to 32 and transformer `max_length` to 256.

> ‚ÑπÔ∏è  You do **not** need to edit Python files for common tweaks‚Äî90 % of day-to-day tuning is config-only.

## üöß Future Expansion Opportunities
The configuration file already reserves space for capabilities that are **not yet wired into the Python code**.  They are safe to keep (the pipeline simply ignores them) and act as a roadmap for growth:

* **hyperparameter_optimization** ‚Äì Optuna search spaces for every model.
* **model_evaluation** ‚Äì statistical tests, confidence intervals, baseline thresholds.
* **model_selection** ‚Äì multi-criteria scorer and ensemble meta-learning.
* **a_b_testing** ‚Äì live traffic splitting and on-line significance testing.
* **model_deployment** ‚Äì BentoML/Docker/Kubernetes hooks for serving.
* **performance_optimization** ‚Äì quantisation, pruning, caching.
* **monitoring** ‚Äì Prometheus metrics exporter.
* **error_handling** ‚Äì retry / circuit-breaker wrappers.
* **compliance** ‚Äì fairness auditing, GDPR data retention.
* **documentation** ‚Äì automatic model-cards and Swagger API docs.

These blocks let you enable new features configuration-first once their corresponding modules are added to `utils/`, `experiment/`, or `serving/` packages.

<!--
# ## üîç External Blind-Set Validation
#
# The framework supports an optional *blind-set* evaluation step to detect data-leakage or over-fitting.
# Enable it via `config/external_validation` (on by default).  The loader reads any JSON list whose
# objects contain `user_message` and `classification` keys.  Metrics are written to the new
# `f1_external` column in `results/results.csv`.
#
# *Replace* `validation_jsons/outside_test_data.json` with your own curated prompts to see how well
# models generalise beyond the training distribution.
-->
