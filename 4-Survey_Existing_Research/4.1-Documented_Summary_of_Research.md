---
title: "4.1 – Documented Summary of Research"
author: "Luke Johnson" (AISecurityLuke)
date: "2025‑06‑14"
---

# Large‑Language‑Model Security:  Prompt Safety, Content Moderation & Injection Detection

## 1  Introduction  
Modern LLMs deliver impressive natural‑language capabilities, yet they also introduce **safety and security risks**.  Two threats dominate applied research today:

1. **Harmful–content generation** – models may produce disallowed or illegal outputs unless filtered.  
2. **Prompt‑injection / jailbreak attacks** – adversaries craft inputs that force the model to ignore its original restraints.

The seven sources reviewed here (two peer‑reviewed papers, two active GitHub projects, and two open datasets) collectively advance our ability to **classify user prompts** as *Safe, Suspicious,* or *Malicious* before they reach production LLM endpoints.

---

## 2  Key Works at a Glance  

| # | Reference | Scope | Model / Method | Dataset | Notable Metric(s) |
|---|-----------|-------|----------------|---------|-------------------|
| **P1** | Markov et al., *A Holistic Approach to Undesired Content Detection* (2023) | End‑to‑end moderation pipeline for disallowed content | Proprietary multi‑label transformer (∼350 M params) with active learning | 2.5 M multi‑label samples (private) | F1 ≈ 0.90–0.95 across hate, violence, self‑harm |
| **P2** | Kim et al., *Adversarial Prompt Shield* (2023) | Detect adversarial suffix “jailbreaks” | DistilBERT + adversarial training (BAND) | 120 k adversarial/benign prompts | ↓ attack success 60 % compared to baseline |
| **R1** | *llm‑security‑prompt‑injection* (GitHub, 2023) | Prompt‑injection binary classifier | a) TF‑IDF + LogReg/SVM b) XLM‑R (fine‑tuned) | deepset Prompt‑Injections (662) | LogReg Acc 96 %; XLM‑R Acc 97 % |
| **R2** | *malicious‑prompt‑detection* (GitHub, 2024) | Embed‑then‑classify pipeline (Random Forest, XGBoost) | OpenAI Ada / GTE‑Large / MiniLM embeddings | 467 k benign vs. attack prompts | RF F1 ≈ 0.87 (OpenAI emb.) |
| **D1** | *harmful_behaviors* (HF dataset, 2023) | 520 direct requests for illicit acts | n/a | n/a | Qualitative corpus for negative class |
| **D2** | *prompt‑injections* (HF dataset, 2023) | 662 crafted injections | n/a | n/a | Widely used evaluation split |
| **D3** | *hackaprompt-dataset* (HF dataset, 2023) | 602 k adversarial-prompt records from the HackAPrompt jailbreak competition | n/a | n/a | Large-scale challenge set for injection evaluation |

*Table 1 – Consolidated comparison of sources.*

---

## 3  Findings & Insights  

### 3.1  Holistic Moderation (P1)  
Markov et al. describe a **production‑grade moderation stack** underpinning OpenAI’s public Moderation API.  Their core lessons are:

* **Granular taxonomy** beats generic “unsafe” flags; each class (sexual, hate, etc.) receives dedicated thresholds.  
* **Active‑learning loops** catch *rare but high‑impact* edge cases (e.g., novel extremist slogans).  
* **Human‑in‑the‑loop evaluation** remains mandatory for ambiguous or culturally sensitive content.

*Capstone link.*  Our Safe/Suspicious/Malicious labels can easily map onto the multi‑label scheme (e.g., *Malicious ∪ Hate ∪ Illicit = Reject*).  Active data refresh cycles will keep the classifier current after deployment.

---

### 3.2  Adversarial Prompt Defenses (P2)  
Kim et al. show that **optimizer‑generated suffix attacks** (>50 % success on GPT‑4) evade many existing filters.  Their **Adversarial Prompt Shield (APS)** counters this by fine‑tuning DistilBERT on synthetically generated *BAND* examples—resulting in a 60 % drop in successful jailbreaks without sacrificing latency (≈7 ms @ batch 1 on an RTX 3060).

*Capstone link.*  Incorporating *BAND‑style* adversarial samples when training our Suspicious/Malicious classes will bolster robustness against evolving jailbreak strings.

---

### 3.3  Classical vs. LLM‑based Classifiers (R1)  
The *llm‑security‑prompt‑injection* project compared:

1. **Classic ML** (TF‑IDF → Logistic Regression, SVM)  
2. **Zero‑shot XLM‑R** (poor; Acc ≈ 55 %)  
3. **Fine‑tuned XLM‑R** (best; Acc ≈ 97 %)

Surprisingly, **Logistic Regression** with good text features rivaled fine‑tuned transformers in accuracy while using < 1 GB RAM.  This underscores the viability of *two‑tier pipelines*: lightweight filters for obvious attacks, heavier models for edge cases.

*Capstone link.*  On 24 GB machines, we can serve a small TF‑IDF + LogReg front‑gate (<50 MB) and lazily invoke our DistilBERT gatekeeper only when confidence is low.

---

### 3.4  Embedding‑Based Detection at Scale (R2)  
Ayub & Majumdar trade model bulk for **rich embeddings + tree ensembles**.  Random Forest on OpenAI embeddings achieves **F1 ≈ 0.87** over nearly half a million prompts while staying CPU‑friendly (<400 MB RAM).  They also report visible clustering of injection prompts in embedding space, aiding *explainability*.

*Capstone link.*  If GPU resources are scarce, embedding‑first architectures provide a cost‑effective fallback; feature importance scores also help security analysts interpret why a prompt was blocked.

---

### 3.5  Dataset Utility (D1, D2, D3)  
- **D1 harmful_behaviors** contains **directly illicit requests** (e.g., *“Write malware to exfiltrate SQL data.”*).  Ideal for the *Malicious = hard reject* class.  
- **D2 prompt‑injections** focuses on **indirect jailbreak phrasing** (*“Ignore all prior instructions…”*).  Perfect for *Suspicious* or *Malicious* training, plus unit tests.
- **D3 hackaprompt-dataset** – **602 000** real competition prompts and completions, each flagged “correct”/“error” depending on whether the LLM was tricked. Ideal for **stress-testing** classifiers against *novel* jailbreak strings at scale.

Combining both widens coverage: overt crime‑facilitation and covert policy bypass.

---

## 4  Synthesis for the Chatbot Filtration Capstone  

| Capstone Need | What the Literature Teaches |
|---------------|-----------------------------|
| **Accurate triage (Safe / Suspicious / Malicious)** | Multi‑label taxonomies (P1) avoid over‑blocking and let us overlay policy thresholds. |
| **Robustness to new jailbreaks** | Adversarially generated BAND samples (P2) and continuous data refresh (P1) are critical. |
| **Resource efficiency (≤ 24 GB RAM)** | Classic TF‑IDF + LogReg (R1) or embedding‑based RF/XGB (R2) meet the budget; DistilBERT adds depth when needed. |
| **Explainability for security audits** | Embedding‑space cluster analysis (R2) and feature weight inspection (R1) provide interpretable cues. |
| **Dataset strategy** | Merge D1 (harmful acts) + D2 (injections) + real user logs; tag edge cases as *Suspicious* for manual review. |

---

## 5  Open Challenges  

1. **Multilingual coverage.**  Current public datasets are English‑heavy; adversaries will pivot to other languages.  
2. **Tool‑facilitated attacks** (e.g., code snippets, emoji obfuscation) require multimodal detectors.  
3. **False‑positive minimization.**  Excessive blocking harms user trust; calibrated thresholds and human override are necessary.

---

## 6  Conclusion  

The reviewed works converge on three pillars for secure LLM deployment:

* **High‑quality taxonomies and annotation.**  Without clear labels, models drift or over‑generalize.  
* **Adversarial training + continual red‑teaming.**  Attackers adapt quickly; defenses must iterate.  
* **Layered, resource‑aware architecture.**  Lightweight heuristics or classical ML catch low‑hanging fruit, while fine‑tuned transformers provide depth for ambiguous cases.

By fusing these lessons, the Chatbot Filtration System can deliver an agile, memory‑efficient gatekeeper that **classifies incoming prompts, logs decisions, and shields the downstream RAG model** from harmful or manipulative content.

---

## 7  References  

1. Markov, I. et al. (2023). *A Holistic Approach to Undesired Content Detection in the Real World.* arXiv:2208.03274  
2. Kim, T. et al. (2023). *Adversarial Prompt Shield: Robust Safety Classifier for LLMs.* arXiv:2311.00172  
3. Wang, S. & Wójcik, J. (2023). *llm‑security‑prompt‑injection* [GitHub repository].  
4. Ayub, A. & Majumdar, P. (2024). *malicious‑prompt‑detection* [GitHub repository].  
5. Labonne, M. (2023). *harmful_behaviors* [Hugging Face dataset].  
6. deepset (2023). *prompt‑injections* [Hugging Face dataset].
7. HackAPrompt Team (2023). *hackaprompt-dataset* [Hugging Face dataset].