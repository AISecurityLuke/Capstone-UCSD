---
title: "4.1 – Documented Publicly Available Code"
author: "Luke Johnson" (AISecurityLuke)
date: "2025‑06‑14"
---

# Publicly Available Code Documentation

### LLM Security Prompt Injection
Repository:https://github.com/sinanw/llm-security-prompt-injection

**Purpose**  
Detect and benchmark prompt-injection attacks on LLMs.

**Key Components**  
- **Dataset** – Deepset Prompt-Injection (English & German), already split into train/test.  
- **Approach 1 – Classical ML** – BERT embeddings + Naive Bayes, Logistic Regression, SVM, Random Forest (~96 % accuracy).  
- **Approach 2 – Zero-shot LLM** – XLM-RoBERTa as a zero-shot classifier (~55 % accuracy).  
- **Approach 3 – Fine-tuned LLM** – Fine-tunes XLM-RoBERTa; best result ~97 % accuracy after 5 epochs.  
- Evaluation scripts output accuracy, F1, and confusion matrices; notebooks automate training runs.

**Relevance to Capstone**  
- Demonstrates that targeted fine-tuning outperforms zero-shot and classical baselines—useful for your *Safe / Suspicious / Malicious* filter.  
- Ready-to-run notebooks and metrics help replicate results quickly and benchmark filtration pipeline.  
- Dataset and visualization code can be mimicked or reused to compare confusion-matrix patterns with API-based classifier.


### Malicious Prompt Detection
Repository: https://github.com/AhsanAyub/malicious-prompt-detection 

**Purpose**  
CAMLIS 2024 project that flags malicious prompts using embedding-based machine-learning classifiers.

**Key Components**  
- **Dataset** – 467 k prompts (≈23 % malicious) aggregated from multiple PI corpora; includes cleaning & deduplication scripts.  
- **Embeddings** – Three vector spaces:  
  - OpenAI `text-embedding-3-small` (API)  
  - `gte-large` (OctoAI hosted)  
  - `all-MiniLM-L6-v2` (local)  
- **Classifiers** – Random Forest, XGBoost, SVM, etc.; RF/XGB yield top performance.  
- **Utilities** – Notebooks for ETL, training, hyper-parameter search, and ROC plotting.

**Relevance to Your Capstone**  
- Shows that lightweight embeddings + classic ML can rival deep encoders—a strong baseline for the filtration system.  
- Provides a large labeled corpus to stress-test threshold settings and handle class imbalance.  
- Modular code swaps in custom embeddings or allows for integration of the trained model directly into `/classification_pipeline`.